{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VisualQuestionAnswering.ipynb","provenance":[],"collapsed_sections":["8DPu8YWiJZ2L","KMGbjMT0FWl-","QrNYLxW_QPKS","rHf0RQHJQntX","XJ1dlg-bjnsA","gqEIRneILoFs","i7c5k73rZ0to","qPtjoFeJhX1z"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jE_81mw-JToN"},"source":["# Homework 3 - Visual Question Answering\r\n","The notebook is divided into several sections:\r\n","\r\n","* Setup - Importing libraries, defining the create_csv function, mounting Drive and unzipping the dataset in the proper Drive directory. Indeed, the notebook was created using the Drive integration with Colab, therefore the main directory is the folder /AN2DL/VisualQuestionAnswering, which was created in advance with the dataset in it.\r\n","* Preparing the data - The training set and the validation set are prepared, preprocessing images and creating the Datasets objects to be used by the models.\r\n","* Models:\r\n","  * First Model\r\n","  * Second Model\r\n","  * Third Model (VGG-16)\r\n","\r\n","  In each model section, the datasets are created, the architecture is defined, the optimization parameters are set, the callbacks are created, the model is trained and finally the predictions on the test set are computed, exporting the results in a csv format."]},{"cell_type":"markdown","metadata":{"id":"8DPu8YWiJZ2L"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"lK2bIyLGJPTG"},"source":["# Importing the necessary libraries and setting the seed(s) to make the code replicable\r\n","from IPython.core.interactiveshell import InteractiveShell\r\n","InteractiveShell.ast_node_interactivity = \"all\"\r\n","\r\n","import os\r\n","import tensorflow as tf\r\n","import numpy as np\r\n","from PIL import Image\r\n","from datetime import datetime\r\n","import json\r\n","\r\n","SEED = 1234\r\n","tf.random.set_seed(SEED)\r\n","np.random.seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVFOw4uAQ_Rz"},"source":["# Defining the create_csv function, which will be used to export the prediction results on the test set\n","def create_csv(results, results_dir='./'):\n","\n","  csv_fname = 'results_'\n","  csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n","\n","  with open(os.path.join(results_dir, csv_fname), 'w') as f:\n","\n","    f.write('Id,Category\\n')\n","\n","    for key, value in results.items():\n","      f.write(key + ',' + str(value) + '\\n') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNQsu5VeJlSt"},"source":["# Mounting Drive to Colab, as the Drive folder /AN2DL/VisualQuestionAnswering is the main directory for this homework\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WwSObyM0Jo91"},"source":["# Unzipping the dataset (named \"anndl-2020-vqa.zip\"), which has to be previously put in the homework directory\r\n","!unzip '/content/drive/My Drive/AN2DL/VisualQuestionAnswering/anndl-2020-vqa.zip'\r\n","\r\n","# Saving the directories for the dataset, the training set and the test set (to be used later)\r\n","cwd = os.getcwd()                                                               # This is the current working directory, in which the dataset has been unzipped\r\n","dataset_dir = os.path.join(cwd, 'VQA_Dataset')                                  # This is the dataset directory, which contains the Images folders, along with the training and test questions json\r\n","images_dir = os.path.join(dataset_dir, 'Images')                                # This is the directory which contains the images for the homework"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMGbjMT0FWl-"},"source":["# Answers labels\r\n","A simple dictionary mapping each possible answer to an integer number."]},{"cell_type":"code","metadata":{"id":"-xJ1mWqbFTBu"},"source":["# This dictionary allows to map each answer (target) of the training set into its corresponding integer\n","# It is exactly the one reported on the homework page on Kaggle\n","labels_dict = {'0': 0,\n","               '1': 1,\n","               '2': 2,\n","               '3': 3,\n","               '4': 4,\n","               '5': 5,\n","               'apple': 6,\n","               'baseball': 7,\n","               'bench': 8,\n","               'bike': 9,\n","               'bird': 10,\n","               'black': 11,\n","               'blanket': 12,\n","               'blue': 13,\n","               'bone': 14,\n","               'book': 15,\n","               'boy': 16,\n","               'brown': 17,\n","               'cat': 18,\n","               'chair': 19,\n","               'couch': 20,\n","               'dog': 21,\n","               'floor': 22,\n","               'food': 23,\n","               'football': 24,\n","               'girl': 25,\n","               'grass': 26,\n","               'gray': 27,\n","               'green': 28,\n","               'left': 29,\n","               'log': 30,\n","               'man': 31,\n","               'monkey bars': 32,\n","               'no': 33,\n","               'nothing': 34,\n","               'orange': 35,\n","               'pie': 36,\n","               'plant': 37,\n","               'playing': 38,\n","               'red': 39,\n","               'right': 40,\n","               'rug': 41,\n","               'sandbox': 42,\n","               'sitting': 43,\n","               'sleeping': 44,\n","               'soccer': 45,\n","               'squirrel': 46,\n","               'standing': 47,\n","               'stool': 48,\n","               'sunny': 49,\n","               'table': 50,\n","               'tree': 51,\n","               'watermelon': 52,\n","               'white': 53,\n","               'wine': 54,\n","               'woman': 55,\n","               'yellow': 56,\n","               'yes': 57}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QrNYLxW_QPKS"},"source":["# Data Preparation (First alternative - Standard)\r\n","Each training json item is split into three different lists (questions, image IDs, answers). Then, questions are tokenized and padded so that they all have the same length. Additionally, answers are converted into integers using the previously defined dictionary. Finally, the lists are split to create separate training and validation data."]},{"cell_type":"code","metadata":{"id":"K_PbdjZblBkG"},"source":["# The training json is loaded.\r\n","# Then, all the questions are put in a list, all the image IDs in another list, and all the answers (targets) in another one.\r\n","question_input = []\r\n","image_ids = []\r\n","output = []\r\n","\r\n","with open('/content/VQA_Dataset/train_questions_annotations.json') as json_file:\r\n","  training_json = json.load(json_file)\r\n","  training_json_list = list(training_json.values())\r\n","\r\n","  for index, item in enumerate(training_json_list):\r\n","    question_input.append(training_json_list[index]['question'])\r\n","    image_ids.append(training_json_list[index]['image_id'])\r\n","    output.append(training_json_list[index]['answer']) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bO-MLo4vlgMc"},"source":["# The questions are passed through a Tokenizer to convert words to integers.\r\n","# Then, padding is applied so that they all have the same length (max_question_length).\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","# Tokenization\r\n","question_tokenizer = Tokenizer()\r\n","question_tokenizer.fit_on_texts(question_input)\r\n","question_tokenized = question_tokenizer.texts_to_sequences(question_input)\r\n","\r\n","question_wtoi = question_tokenizer.word_index\r\n","print('Unique words in the questions:', len(question_wtoi))\r\n","\r\n","max_question_length = max(len(sentence) for sentence in question_tokenized)\r\n","print('Maximum questions length:', max_question_length)\r\n","\r\n","# Padding\r\n","question_encoder_inputs = pad_sequences(question_tokenized, maxlen=max_question_length)\r\n","print(\"Shape of the padded questions:\", question_encoder_inputs.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9lcPeGu1m79x"},"source":["# Converting answers into labels, using the dictionary previously defined\r\n","output_number = []\r\n","\r\n","for o in output:\r\n","  output_number.append(labels_dict[o])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9neZKC5QQw5"},"source":["# Splitting tokenized+padded questions, image IDs and answers (targets) labels into training and validation sets.\r\n","# In particular, image IDs are written into two separate txt files, to be used later in the CustomDataset object.\r\n","# The split ratio used here is 70% for training and 30% for validation.\r\n","train_question_input = []\r\n","valid_question_input = []\r\n","train_images_names = open('/content/VQA_Dataset/train_images_names.txt', 'a')\r\n","valid_images_names = open('/content/VQA_Dataset/valid_images_names.txt', 'a')\r\n","train_output = []\r\n","valid_output = []\r\n","\r\n","counter = 1\r\n","for item in question_encoder_inputs:\r\n","  if (counter < 0.7 * len(question_encoder_inputs)):\r\n","    train_question_input.append(item)\r\n","  else:\r\n","    valid_question_input.append(item)\r\n","  counter = counter + 1\r\n","\r\n","counter = 1\r\n","for item in output_number:\r\n","  if (counter < 0.7 * len(output_number)):\r\n","    train_output.append(item)\r\n","  else:\r\n","    valid_output.append(item)\r\n","  counter = counter + 1\r\n","\r\n","counter = 1\r\n","for item in image_ids:\r\n","  if (counter < 0.7 * len(image_ids)):\r\n","    _ = train_images_names.write(item)\r\n","    if (counter < (0.7 * len(image_ids)) - 1):\r\n","      _ = train_images_names.write('\\n')\r\n","  else:\r\n","    _ = valid_images_names.write(item)\r\n","    if (counter < len(image_ids)):\r\n","      _ = valid_images_names.write('\\n')\r\n","  counter = counter + 1\r\n","\r\n","train_images_names.close()\r\n","valid_images_names.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHf0RQHJQntX"},"source":["# Data Preparation (Second alternative - Split by category)\r\n","Training json items are divided according to the category they belong to, after having converted answers into integers using the previously defined dictionary. Then, each item is split into three different lists (for each category, for a total of nine lists) to get questions, image IDs and answers. Finally, questions are tokenized and padded so they all have the same length and everything is split into training and validation data, respecting the category distribution."]},{"cell_type":"code","metadata":{"id":"Zd9CFUn-Qz1y"},"source":["# The training json is loaded and its data is converted into a list for easier use\r\n","with open('/content/VQA_Dataset/train_questions_annotations.json') as json_file:\r\n","  training_json = json.load(json_file)\r\n","  training_json = list(training_json.values())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jZTe_XYPRGOh"},"source":["# Converting every answer in the training list into the corresponding integer, according to the dictionary previously defined\r\n","for item in training_json:\r\n","  item['answer'] = labels_dict[item['answer']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q578KphJSnA8"},"source":["# Dividing the items in the training list into the three categories (yes/no, counting, other)\r\n","yes_no_items = []\r\n","counting_items = []\r\n","other_items = []\r\n","\r\n","for item in training_json:\r\n","  if (item['answer'] == 33 or item['answer'] == 57):\r\n","    yes_no_items.append(item)\r\n","  elif (item['answer'] <= 5):\r\n","    counting_items.append(item)\r\n","  else:\r\n","    other_items.append(item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAdH52F5xvS5"},"source":["# Shuffling the three category lists using a fixed seed\r\n","import random\r\n","\r\n","random.Random(SEED).shuffle(yes_no_items)\r\n","random.Random(SEED).shuffle(counting_items)\r\n","random.Random(SEED).shuffle(other_items)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eIeOYOFkTXZ-"},"source":["# Dividing each category list into questions, image IDs and answers lists\r\n","yes_no_questions = []\r\n","yes_no_image_ids = []\r\n","yes_no_answers = []\r\n","counting_questions = []\r\n","counting_image_ids = []\r\n","counting_answers = []\r\n","other_questions = []\r\n","other_image_ids = []\r\n","other_answers = []\r\n","\r\n","for item in yes_no_items:\r\n","  yes_no_questions.append(item['question'])\r\n","  yes_no_image_ids.append(item['image_id'])\r\n","  yes_no_answers.append(item['answer'])\r\n","\r\n","for item in counting_items:\r\n","  counting_questions.append(item['question'])\r\n","  counting_image_ids.append(item['image_id'])\r\n","  counting_answers.append(item['answer'])\r\n","\r\n","for item in other_items:\r\n","  other_questions.append(item['question'])\r\n","  other_image_ids.append(item['image_id'])\r\n","  other_answers.append(item['answer'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjqrVJWUU3h7"},"source":["# Splitting data into training and validation, using a 70/30 split ratio.\r\n","# Data distribution is maintained, since 70% of each category is put in the training set and 30% in the validation set.\r\n","TRAIN_PERC = 0.7\r\n","\r\n","train_questions = []\r\n","valid_questions = []\r\n","train_image_ids = []\r\n","valid_image_ids = []\r\n","train_answers = []\r\n","valid_answers = []\r\n","\r\n","# Training questions\r\n","train_questions.extend(yes_no_questions[:round(TRAIN_PERC * len(yes_no_questions))])\r\n","train_questions.extend(counting_questions[:round(TRAIN_PERC * len(counting_questions))])\r\n","train_questions.extend(other_questions[:round(TRAIN_PERC * len(other_questions))])\r\n","\r\n","# Validation questions\r\n","valid_questions.extend(yes_no_questions[round(TRAIN_PERC * len(yes_no_questions)):])\r\n","valid_questions.extend(counting_questions[round(TRAIN_PERC * len(counting_questions)):])\r\n","valid_questions.extend(other_questions[round(TRAIN_PERC * len(other_questions)):])\r\n","\r\n","# Training answers (targets)\r\n","train_answers.extend(yes_no_answers[:round(TRAIN_PERC * len(yes_no_answers))])\r\n","train_answers.extend(counting_answers[:round(TRAIN_PERC * len(counting_answers))])\r\n","train_answers.extend(other_answers[:round(TRAIN_PERC * len(other_answers))])\r\n","\r\n","# Validation answers (targets)\r\n","valid_answers.extend(yes_no_answers[round(TRAIN_PERC * len(yes_no_answers)):])\r\n","valid_answers.extend(counting_answers[round(TRAIN_PERC * len(counting_answers)):])\r\n","valid_answers.extend(other_answers[round(TRAIN_PERC * len(other_answers)):])\r\n","\r\n","# Training image IDs\r\n","train_image_ids.extend(yes_no_image_ids[:round(TRAIN_PERC * len(yes_no_image_ids))])\r\n","train_image_ids.extend(counting_image_ids[:round(TRAIN_PERC * len(counting_image_ids))])\r\n","train_image_ids.extend(other_image_ids[:round(TRAIN_PERC * len(other_image_ids))])\r\n","\r\n","# Validation image IDs\r\n","valid_image_ids.extend(yes_no_image_ids[round(TRAIN_PERC * len(yes_no_image_ids)):])\r\n","valid_image_ids.extend(counting_image_ids[round(TRAIN_PERC * len(counting_image_ids)):])\r\n","valid_image_ids.extend(other_image_ids[round(TRAIN_PERC * len(other_image_ids)):])\r\n","\r\n","# Writing training image IDs into a txt file, to be used by the CustomDataset later\r\n","with open('/content/VQA_Dataset/train_images_names.txt', 'a') as train_images_txt:\r\n","  for idx, item in enumerate(train_image_ids, start=1):\r\n","    _ = train_images_txt.write(item)\r\n","    if (idx < len(train_image_ids)):\r\n","      _ = train_images_txt.write(\"\\n\")\r\n","\r\n","# Writing validation image IDs into a txt file, to be used by the CustomDataset later\r\n","with open('/content/VQA_Dataset/valid_images_names.txt', 'a') as valid_images_txt:\r\n","  for idx, item in enumerate(valid_image_ids, start=1):\r\n","    _ = valid_images_txt.write(item)\r\n","    if (idx < len(valid_image_ids)):\r\n","      _ = valid_images_txt.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPr6tMFyUcCY"},"source":["# The questions are put together into a single list and passed through a Tokenizer to convert words to integers.\r\n","# Then, padding is applied so that they all have the same length (max_question_length).\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","# Merging training questions and validation questions into a single list\r\n","question_input = []\r\n","question_input.extend(train_questions)\r\n","question_input.extend(valid_questions)\r\n","\r\n","# Tokenization\r\n","question_tokenizer = Tokenizer()\r\n","question_tokenizer.fit_on_texts(question_input)\r\n","question_tokenized = question_tokenizer.texts_to_sequences(question_input)\r\n","\r\n","question_wtoi = question_tokenizer.word_index\r\n","print('Unique words in the questions:', len(question_wtoi))\r\n","\r\n","max_question_length = max(len(sentence) for sentence in question_tokenized)\r\n","print('Maximum questions length:', max_question_length)\r\n","\r\n","# Padding\r\n","question_encoder_inputs = pad_sequences(question_tokenized, maxlen=max_question_length)\r\n","print(\"Shape of the padded questions:\", question_encoder_inputs.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"75Ph-R7svfxH"},"source":["# Splitting the tokenized+padded questions into training and validation questions, using the same split ratio as before.\r\n","# Additionally, training and validation answers are renamed just to avoid modifying the dataset structure.\r\n","train_question_input = []\r\n","valid_question_input = []\r\n","train_output = train_answers\r\n","valid_output = valid_answers\r\n","\r\n","train_question_input = question_encoder_inputs[:round(TRAIN_PERC * len(question_encoder_inputs))]\r\n","valid_question_input = question_encoder_inputs[round(TRAIN_PERC * len(question_encoder_inputs)):]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJ1dlg-bjnsA"},"source":["# Custom Dataset"]},{"cell_type":"code","metadata":{"id":"bQhZVc1DX8sJ"},"source":["# A CustomDataset object is defined, to be used later in each model.\r\n","# It selects the right data (training/validation) as specified in the input.\r\n","# Additionally, the input parameters must include the right questions and answers (targets) prepared before, according to the selected subset.\r\n","# A proper preprocessing function can also be included.\r\n","# The output of the CustomDataset is composed by a tuple ((image, question), answer).\r\n","\r\n","class CustomDataset(tf.keras.utils.Sequence):\r\n","\r\n","  def __init__(self, dataset_dir, which_subset, train_question_input=None, valid_question_input=None, train_output=None, valid_output=None, preprocessing_function=None, out_shape=None):\r\n","    if which_subset == 'training':\r\n","      subset_file = os.path.join(dataset_dir, 'train_images_names.txt')\r\n","      self.question_input = train_question_input\r\n","      self.output = train_output\r\n","    elif which_subset == 'validation':\r\n","      subset_file = os.path.join(dataset_dir, 'valid_images_names.txt')\r\n","      self.question_input = valid_question_input\r\n","      self.output = valid_output\r\n","    \r\n","    with open(subset_file, 'r') as f:\r\n","      lines = f.readlines()\r\n","    \r\n","    subset_filenames = []\r\n","    for line in lines:\r\n","      subset_filenames.append(line.strip()) \r\n","\r\n","    self.which_subset = which_subset\r\n","    self.dataset_dir = dataset_dir\r\n","    self.subset_filenames = subset_filenames\r\n","    self.preprocessing_function = preprocessing_function\r\n","    self.out_shape = out_shape\r\n","\r\n","  def __len__(self):\r\n","    return len(self.subset_filenames)\r\n","\r\n","  def __getitem__(self, index):\r\n","    curr_filename = self.subset_filenames[index]\r\n","    img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.png')).convert('RGB').resize(self.out_shape)\r\n","    img_arr = np.array(img)\r\n","\r\n","    curr_question = self.question_input[index]\r\n","    curr_output = self.output[index]\r\n","    \r\n","    if self.preprocessing_function is not None:\r\n","      img_arr = self.preprocessing_function(img_arr)\r\n","\r\n","    return ((img_arr, curr_question), curr_output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gqEIRneILoFs"},"source":["# First Model"]},{"cell_type":"code","metadata":{"id":"WU6SunWUTo2W"},"source":["# Creating the CustomDataset object for the model, followed by the actual dataset that will be used for training\r\n","\r\n","# Image dimensions to work with\r\n","img_h = 256\r\n","img_w = 256\r\n","\r\n","# Batch size\r\n","bs = 32\r\n","\r\n","# Training Dataset\r\n","dataset = CustomDataset('/content/VQA_Dataset', 'training', train_question_input=train_question_input, train_output=train_output, out_shape=[img_w, img_h])\r\n","\r\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\r\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\r\n","                                               output_shapes=(([img_h, img_w, 3], [max_question_length]), ()))\r\n","\r\n","train_dataset = train_dataset.batch(bs)                                                          \r\n","train_dataset = train_dataset.repeat()\r\n","\r\n","# Validation Dataset\r\n","dataset_valid = CustomDataset('/content/VQA_Dataset', 'validation', valid_question_input=valid_question_input, valid_output=valid_output, out_shape=[img_w, img_h])\r\n","\r\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\r\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\r\n","                                               output_shapes=(([img_h, img_w, 3], [max_question_length]), ()))\r\n","\r\n","valid_dataset = valid_dataset.batch(bs)\r\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0t_DZ7hsLrvP"},"source":["# Importing Keras libraries for easier use\r\n","from keras.layers import Conv2D, MaxPooling2D, Flatten\r\n","from keras.layers import Input, LSTM, Embedding, Dense\r\n","from keras.models import Model, Sequential\r\n","\r\n","# Defining the CNN part, which deals with the image\r\n","# It is composed by two convolutional layers using 64 3x3 filters, a 2x2 MaxPooling, two convolutional layers using 128 3x3 filters, a MaxPooling,\r\n","# three convolutional layers using 256 3z3 filters, a MaxPooling, and the final Flatten layer\r\n","vision_model = Sequential()\r\n","vision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_h, img_w, 3)))\r\n","vision_model.add(Conv2D(64, (3, 3), activation='relu'))\r\n","vision_model.add(MaxPooling2D((2, 2)))\r\n","vision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\r\n","vision_model.add(Conv2D(128, (3, 3), activation='relu'))\r\n","vision_model.add(MaxPooling2D((2, 2)))\r\n","vision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\r\n","vision_model.add(Conv2D(256, (3, 3), activation='relu'))\r\n","vision_model.add(Conv2D(256, (3, 3), activation='relu'))\r\n","vision_model.add(MaxPooling2D((2, 2)))\r\n","vision_model.add(Flatten())\r\n","\r\n","image_input = Input(shape=(img_h, img_w, 3))\r\n","encoded_image = vision_model(image_input)\r\n","\r\n","# Defining the RNN part, which deals with the question\r\n","# It is composed by an Enbedding of the question, followed by a single LSTM layer with 256 units\r\n","question_input_model = Input(shape=[max_question_length], dtype='int32')\r\n","embedded_question = Embedding(input_dim=len(question_wtoi)+1, output_dim=256, input_length=max_question_length)(question_input_model)\r\n","encoded_question = LSTM(256)(embedded_question)\r\n","\r\n","# Comcatenating the CNN and the RNN parts to get the output\r\n","merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\r\n","output_model = Dense(len(labels_dict), activation='softmax')(merged)\r\n","vqa_model = Model(inputs=[image_input, question_input_model], outputs=output_model)\r\n","\r\n","# Printing out a summary of the network\r\n","vqa_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIlDus3kIniI"},"source":["# Optimization parameters\r\n","\r\n","# Loss function\r\n","loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n","\r\n","# Learning rate and Optimizer\r\n","lr = 1e-3\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","\r\n","# Validation metrics\r\n","metrics = ['accuracy']\r\n","\r\n","# Compile Model\r\n","vqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6E91KzwtI1JI"},"source":["# Setting up the callbacks and Early Stopping\r\n","# The purpose of this piece of code is to create a \"vqa_experiments\" folder inside the directory of this homework (if not already created).\r\n","# Inside it, it creates a folder called \"VQA_\" followed by the date and the time of execution, to recognize the experiment.\r\n","# Then, it sets up the callback for the training of the model, saving the model weights after each epoch inside the previously mentioned folder, only if the model improved in accuracy on the Validation set.\r\n","# Moreover, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\r\n","# Finally, the model is fitted using the training and validation data defined before.\r\n","\r\n","# Creating the \"multiclass_segmentation_experiments\" folder if not already created\r\n","exps_dir = os.path.join(cwd, 'drive/My Drive/AN2DL/VisualQuestionAnswering/', 'vqa_experiments')\r\n","if not os.path.exists(exps_dir):\r\n","  os.makedirs(exps_dir)\r\n","\r\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n","\r\n","# Creating the folder in which the model weights will be saved\r\n","model_name = 'VQA'\r\n","\r\n","exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\r\n","if not os.path.exists(exp_dir):\r\n","  os.makedirs(exp_dir)\r\n","\r\n","# Setting up the callback to save the model weights after each epoch only if there is an improvement in term of validation accuracy  \r\n","callbacks = []\r\n","\r\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n","if not os.path.exists(ckpt_dir):\r\n","  os.makedirs(ckpt_dir)\r\n","\r\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_dir, \r\n","                                                   monitor='val_accuracy',\r\n","                                                   mode='max',\r\n","                                                   verbose=0,\r\n","                                                   save_best_only=True,\r\n","                                                   save_weights_only=True)\r\n","callbacks.append(ckpt_callback)\r\n","\r\n","# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\r\n","early_stop = True\r\n","if early_stop:\r\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\r\n","  callbacks.append(es_callback)\r\n","\r\n","# Fitting the model\r\n","# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\r\n","vqa_model.fit(train_dataset,\r\n","              epochs=100,\r\n","              steps_per_epoch=(len(dataset) // bs),\r\n","              validation_data=valid_dataset,\r\n","              validation_steps=(len(dataset_valid) // bs),\r\n","              callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_n5XLBopEMXE"},"source":["# Loading the best weights of the trained model\r\n","full_path = os.path.join('/content/drive/My Drive/AN2DL/VisualQuestionAnswering/vqa_experiments', exp_dir)\r\n","latest = tf.train.latest_checkpoint(full_path)\r\n","vqa_model.load_weights(latest)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDqBfcXbI-MY"},"source":["# Checking how the model predictions on the validation set\n","import time\n","from matplotlib import cm\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","iterator = iter(valid_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCNneRDYIr8s"},"source":["# Visualizing a target for an item in the validation set and its corresponding model prediction\n","(image, question), target = next(iterator)\n","\n","image = image[0]\n","question = question[0]\n","target = target[0]\n","\n","x = tf.expand_dims(image,0)\n","y = tf.expand_dims(question, 0)\n","out_sigmoid = vqa_model.predict([x,y])\n","predicted_class = tf.argmax(out_sigmoid, -1)\n","\n","print(\"Target:\")\n","tf.print(target)\n","print(\"Prediction:\")\n","print(predicted_class.numpy()[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZf1YPrQ1A43"},"source":["# Loading the json file related to the test set, saving the items IDs, the questions and the images IDs in some lists\n","id_test = []\n","question_input_test = []\n","image_ids_test = []\n","\n","with open('/content/VQA_Dataset/test_questions.json') as json_file:\n","  test_json = json.load(json_file)                                         \n","\n","  for item in test_json:\n","    id_test.append(item)\n","    question_input_test.append(test_json[item]['question'])\n","    image_ids_test.append(test_json[item]['image_id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HY6ONQjDMKu4"},"source":["# Using the same Tokenizer of the training part to convert the words in the questions to integers\n","question_tokenized_test = question_tokenizer.texts_to_sequences(question_input_test)\n","\n","# Padding to the same max question length used for training\n","question_encoder_inputs_test = pad_sequences(question_tokenized_test, maxlen=max_question_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_lhBEsmPhfe"},"source":["# Computing the predictions on the test set, giving each image with the related question to the model and computing the argmax of the output\n","# In particular, each image is first resized to the same dimensions used during training\n","# The predicted output is then saved into the results_test list\n","results_test = []\n","\n","for index, question in enumerate(question_encoder_inputs_test):\n","  img_test = Image.open(os.path.join(images_dir, image_ids_test[index] + '.png')).convert('RGB').resize((img_w, img_h))\n","  img_arr_test = np.array(img_test)\n","  img_arr_test = preprocess_input(img_arr_test)\n","  img_arr_test = tf.expand_dims(img_arr_test, 0)\n","  \n","  question = tf.expand_dims(question, 0)\n","  \n","  prediction = vqa_model.predict([img_arr_test, question])\n","  results_test.append(tf.argmax(prediction, -1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kvOUxS3RIEZ"},"source":["# Creating the dictionary to be written in the output csv file\n","# It will contain the ID of each test item as the key, and the corresponding predicted output as the value\n","dictionary = {}\n","\n","for index, id in enumerate(id_test):\n","  dictionary[id] = results_test[index].numpy()[0]\n","\n","# Exporting the dictionary created into a csv file\n","create_csv(dictionary, '/content/drive/My Drive/AN2DL/VisualQuestionAnswering/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i7c5k73rZ0to"},"source":["# Second Model"]},{"cell_type":"code","metadata":{"id":"Je91eU4diPYv"},"source":["# Creating the CustomDataset object for the model, followed by the actual dataset that will be used for training\r\n","\r\n","# Image dimensions to work with\r\n","img_h = 256\r\n","img_w = 256\r\n","\r\n","# Batch size\r\n","bs = 32\r\n","\r\n","# Training Dataset\r\n","dataset = CustomDataset('/content/VQA_Dataset', 'training', train_question_input=train_question_input, train_output=train_output, out_shape=[img_w, img_h])\r\n","\r\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\r\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\r\n","                                               output_shapes=(([img_h, img_w, 3], [max_question_length]), ()))\r\n","\r\n","train_dataset = train_dataset.batch(bs)                                                          \r\n","train_dataset = train_dataset.repeat()\r\n","\r\n","# Validation Dataset\r\n","dataset_valid = CustomDataset('/content/VQA_Dataset', 'validation', valid_question_input=valid_question_input, valid_output=valid_output, out_shape=[img_w, img_h])\r\n","\r\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\r\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\r\n","                                               output_shapes=(([img_h, img_w, 3], [max_question_length]), ()))\r\n","\r\n","valid_dataset = valid_dataset.batch(bs)\r\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-_-5hR3Z25W"},"source":["# Importing Keras libraries for easier use\r\n","from keras.layers import Conv2D, MaxPooling2D, Flatten\r\n","from keras.layers import Input, LSTM, Embedding, Dense\r\n","from keras.models import Model, Sequential\r\n","\r\n","# Defining the CNN part, which deals with the image\r\n","# It is composed by two convolutional layers using 64 3x3 filters, a 2x2 MaxPooling, two convolutional layers using 128 3x3 filters, a MaxPooling,\r\n","# three convolutional layers using 256 3z3 filters, a MaxPooling, and the final Flatten layer\r\n","vision_model = Sequential()\r\n","vision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_h, img_w, 3)))\r\n","vision_model.add(Conv2D(64, (3, 3), activation='relu'))\r\n","vision_model.add(MaxPooling2D((2, 2)))\r\n","vision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\r\n","vision_model.add(Conv2D(128, (3, 3), activation='relu'))\r\n","vision_model.add(MaxPooling2D((2, 2)))\r\n","vision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\r\n","vision_model.add(Conv2D(256, (3, 3), activation='relu'))\r\n","vision_model.add(Conv2D(256, (3, 3), activation='relu'))\r\n","vision_model.add(MaxPooling2D((2, 2)))\r\n","vision_model.add(Flatten())\r\n","\r\n","image_input = Input(shape=(img_h, img_w, 3))\r\n","encoded_image = vision_model(image_input)\r\n","\r\n","# Defining the RNN part, which deals with the question\r\n","# It is composed by an Enbedding of the question, followed by a three LSTM layers, each one with 256 units\r\n","question_input_model = Input(shape=[max_question_length], dtype='int32')\r\n","embedded_question = Embedding(input_dim=len(question_wtoi)+1, output_dim=256, input_length=max_question_length)(question_input_model)\r\n","encoded_question_1 = LSTM(256, return_sequences=True)(embedded_question)\r\n","encoded_question_2 = LSTM(256, return_sequences=True)(encoded_question_1)\r\n","encoded_question_3 = LSTM(256)(encoded_question_2)\r\n","\r\n","# Comcatenating the CNN and the RNN parts to get the output\r\n","merged = tf.keras.layers.concatenate([encoded_question_3, encoded_image])\r\n","output_model = Dense(len(labels_dict), activation='softmax')(merged)\r\n","vqa_model_2 = Model(inputs=[image_input, question_input_model], outputs=output_model)\r\n","\r\n","# Printing out a summary of the network\r\n","vqa_model_2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMdyz-CSZ6uh"},"source":["# Optimization parameters\r\n","\r\n","# Loss function\r\n","loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n","\r\n","# Learning rate and Optimizer\r\n","lr = 1e-3\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","\r\n","# Validation metrics\r\n","metrics = ['accuracy']\r\n","\r\n","# Compile Model\r\n","vqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCO9y__0Z8QD"},"source":["# Setting up the callbacks and Early Stopping\r\n","# The purpose of this piece of code is to create a \"vqa_experiments\" folder inside the directory of this homework (if not already created).\r\n","# Inside it, it creates a folder called \"VQA_2_\" followed by the date and the time of execution, to recognize the experiment.\r\n","# Then, it sets up the callback for the training of the model, saving the model weights after each epoch inside the previously mentioned folder, only if the model improved in accuracy on the Validation set.\r\n","# Moreover, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\r\n","# Finally, the model is fitted using the training and validation data defined before.\r\n","\r\n","# Creating the \"multiclass_segmentation_experiments\" folder if not already created\r\n","exps_dir = os.path.join(cwd, 'drive/My Drive/AN2DL/VisualQuestionAnswering/', 'vqa_experiments')\r\n","if not os.path.exists(exps_dir):\r\n","  os.makedirs(exps_dir)\r\n","\r\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n","\r\n","# Creating the folder in which the model weights will be saved\r\n","model_name = 'VQA_2'\r\n","\r\n","exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\r\n","if not os.path.exists(exp_dir):\r\n","  os.makedirs(exp_dir)\r\n","\r\n","# Setting up the callback to save the model weights after each epoch only if there is an improvement in term of validation accuracy\r\n","callbacks = []\r\n","\r\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n","if not os.path.exists(ckpt_dir):\r\n","  os.makedirs(ckpt_dir)\r\n","\r\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_dir, \r\n","                                                   monitor='val_accuracy',\r\n","                                                   mode='max',\r\n","                                                   verbose=0,\r\n","                                                   save_best_only=True,\r\n","                                                   save_weights_only=True)\r\n","callbacks.append(ckpt_callback)\r\n","\r\n","# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\r\n","early_stop = True\r\n","if early_stop:\r\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\r\n","  callbacks.append(es_callback)\r\n","\r\n","# Fitting the model\r\n","# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\r\n","vqa_model_2.fit(train_dataset,\r\n","                epochs=100,\r\n","                steps_per_epoch=(len(dataset) // bs),\r\n","                validation_data=valid_dataset,\r\n","                validation_steps=(len(dataset_valid) // bs),\r\n","                callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IR4OMTCKhNhC"},"source":["# Loading the best weights of the trained model\r\n","full_path = os.path.join('/content/drive/My Drive/AN2DL/VisualQuestionAnswering/vqa_experiments', exp_dir)\r\n","latest = tf.train.latest_checkpoint(full_path)\r\n","vqa_model_2.load_weights(latest)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjbVKQELGsT9"},"source":["# Checking how the model predictions on the validation set\r\n","import time\r\n","from matplotlib import cm\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline\r\n","\r\n","iterator = iter(valid_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LBqCEiZGvLJ"},"source":["# Visualizing a target for an item in the validation set and its corresponding model prediction\r\n","(image, question), target = next(iterator)\r\n","\r\n","image = image[0]\r\n","question = question[0]\r\n","target = target[0]\r\n","\r\n","x = tf.expand_dims(image,0)\r\n","y = tf.expand_dims(question, 0)\r\n","out_sigmoid = vqa_model_2.predict([x,y])\r\n","predicted_class = tf.argmax(out_sigmoid, -1)\r\n","\r\n","print(\"Target:\")\r\n","tf.print(target)\r\n","print(\"Prediction:\")\r\n","print(predicted_class.numpy()[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sv4TqsdBhRg5"},"source":["# Loading the json file related to the test set, saving the items IDs, the questions and the images IDs in some lists\r\n","id_test = []\r\n","question_input_test = []\r\n","image_ids_test = []\r\n","\r\n","with open('/content/VQA_Dataset/test_questions.json') as json_file:\r\n","  test_json = json.load(json_file)                                         \r\n","\r\n","  for item in test_json:\r\n","    id_test.append(item)\r\n","    question_input_test.append(test_json[item]['question'])\r\n","    image_ids_test.append(test_json[item]['image_id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfYfHCQwhTer"},"source":["# Using the same Tokenizer of the training part to convert the words in the questions to integers\r\n","question_tokenized_test = question_tokenizer.texts_to_sequences(question_input_test)\r\n","\r\n","# Padding to the same max question length used for training\r\n","question_encoder_inputs_test = pad_sequences(question_tokenized_test, maxlen=max_question_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ivGakq4jhVHJ"},"source":["# Computing the predictions on the test set, giving each image with the related question to the model and computing the argmax of the output\r\n","# In particular, each image is first resized to the same dimensions used during training\r\n","# The predicted output is then saved into the results_test list\r\n","results_test = []\r\n","\r\n","for index, question in enumerate(question_encoder_inputs_test):\r\n","  img_test = Image.open(os.path.join(images_dir, image_ids_test[index] + '.png')).convert('RGB').resize((img_w, img_h))\r\n","  img_arr_test = np.array(img_test)\r\n","  img_arr_test = preprocess_input(img_arr_test)\r\n","  img_arr_test = tf.expand_dims(img_arr_test, 0)\r\n","  \r\n","  question = tf.expand_dims(question, 0)\r\n","  \r\n","  prediction = vqa_model_2.predict([img_arr_test, question])\r\n","  results_test.append(tf.argmax(prediction, -1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qN_Niee7hWgN"},"source":["# Creating the dictionary to be written in the output csv file\r\n","# It will contain the ID of each test item as the key, and the corresponding predicted output as the value\r\n","dictionary = {}\r\n","\r\n","for index, id in enumerate(id_test):\r\n","  dictionary[id] = results_test[index].numpy()[0]\r\n","\r\n","# Exporting the dictionary created into a csv file\r\n","create_csv(dictionary, '/content/drive/My Drive/AN2DL/VisualQuestionAnswering/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qPtjoFeJhX1z"},"source":["# Third Model (using VGG)"]},{"cell_type":"code","metadata":{"id":"XFjqKD6miRna"},"source":["# Creating the CustomDataset object for the model, followed by the actual dataset that will be used for training\r\n","from tensorflow.keras.applications.vgg16 import preprocess_input\r\n","\r\n","# Image dimensions to work with\r\n","img_h = 400\r\n","img_w = 700\r\n","\r\n","# Batch size\r\n","bs = 16\r\n","\r\n","# Training Dataset\r\n","dataset = CustomDataset('/content/VQA_Dataset', 'training', train_question_input=train_question_input, train_output=train_output, preprocessing_function=preprocess_input, out_shape=[img_w, img_h])\r\n","\r\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\r\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\r\n","                                               output_shapes=(([img_h, img_w, 3], [max_question_length]), ()))\r\n","\r\n","train_dataset = train_dataset.batch(bs)                                                          \r\n","train_dataset = train_dataset.repeat()\r\n","\r\n","# Validation Dataset\r\n","dataset_valid = CustomDataset('/content/VQA_Dataset', 'validation', valid_question_input=valid_question_input, valid_output=valid_output, preprocessing_function=preprocess_input, out_shape=[img_w, img_h])\r\n","\r\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\r\n","                                               output_types=((tf.float32, tf.int32), tf.int32),\r\n","                                               output_shapes=(([img_h, img_w, 3], [max_question_length]), ()))\r\n","\r\n","valid_dataset = valid_dataset.batch(bs)\r\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"29Ge-N2Rhswo"},"source":["# Importing the VGG-16 architecture, without the top part\r\n","vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\r\n","\r\n","# Setting the Fine-Tuning parameter\r\n","finetuning = True\r\n","\r\n","if finetuning:\r\n","  freeze_until = 13\r\n","  for layer in vgg.layers[:freeze_until]:\r\n","    layer.trainable = False\r\n","else:\r\n","  vgg.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxS_HHYkhaqd"},"source":["# Importing Keras libraries for easier use\r\n","from keras.layers import Conv2D, MaxPooling2D, Flatten\r\n","from keras.layers import Input, LSTM, Embedding, Dense\r\n","from keras.models import Model, Sequential\r\n","\r\n","# Defining the CNN part, which deals with the image\r\n","# It consists in the VGG-16 architecture imported before, followed of course by a Flatten layer\r\n","vision_model = Sequential()\r\n","vision_model.add(vgg)\r\n","vision_model.add(Flatten())\r\n","\r\n","image_input = Input(shape=(img_h, img_w, 3))\r\n","encoded_image = vision_model(image_input)\r\n","\r\n","# Defining the RNN part, which deals with the question\r\n","# It consists in an Embedding, followed by three LSTM layers, each one with 256 units\r\n","question_input_model = Input(shape=[max_question_length], dtype='int32')\r\n","embedded_question = Embedding(input_dim=len(question_wtoi)+1, output_dim=256, input_length=max_question_length)(question_input_model)\r\n","encoded_question_1 = LSTM(256, return_sequences=True)(embedded_question)\r\n","encoded_question_2 = LSTM(256, return_sequences=True)(encoded_question_1)\r\n","encoded_question_3 = LSTM(256)(encoded_question_2)\r\n","\r\n","# Comcatenating the CNN and the RNN parts to get the output\r\n","merged = tf.keras.layers.concatenate([encoded_question_3, encoded_image])\r\n","output_model = Dense(len(labels_dict), activation='softmax')(merged)\r\n","vqa_model_3 = Model(inputs=[image_input, question_input_model], outputs=output_model)\r\n","\r\n","# Printing out a summary of the network\r\n","vqa_model_3.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9qr_RNfif9r"},"source":["# Optimization parameters\r\n","\r\n","# Loss function\r\n","loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n","\r\n","# Learning rate and Optimizer\r\n","lr = 1e-3\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","\r\n","# Validation metrics\r\n","metrics = ['accuracy']\r\n","\r\n","# Compile Model\r\n","vqa_model_3.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BefI0q7liiRi"},"source":["# Setting up the callbacks and Early Stopping\r\n","# The purpose of this piece of code is to create a \"vqa_experiments\" folder inside the directory of this homework (if not already created).\r\n","# Inside it, it creates a folder called \"VQA_3_\" followed by the date and the time of execution, to recognize the experiment.\r\n","# Then, it sets up the callback for the training of the model, saving the model weights after each epoch inside the previously mentioned folder, only if the model improved in accuracy on the Validation set.\r\n","# Moreover, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\r\n","# Finally, the model is fitted using the training and validation data defined before.\r\n","\r\n","# Creating the \"multiclass_segmentation_experiments\" folder if not already created\r\n","exps_dir = os.path.join(cwd, 'drive/My Drive/AN2DL/VisualQuestionAnswering/', 'vqa_experiments')\r\n","if not os.path.exists(exps_dir):\r\n","  os.makedirs(exps_dir)\r\n","\r\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n","\r\n","# Creating the folder in which the model weights will be saved\r\n","model_name = 'VQA_3'\r\n","\r\n","exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\r\n","if not os.path.exists(exp_dir):\r\n","  os.makedirs(exp_dir)\r\n","\r\n","# Setting up the callback to save the model weights after each epoch only if there is an improvement in term of validation accuracy    \r\n","callbacks = []\r\n","\r\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n","if not os.path.exists(ckpt_dir):\r\n","  os.makedirs(ckpt_dir)\r\n","\r\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_dir, \r\n","                                                   monitor='val_accuracy',\r\n","                                                   mode='max',\r\n","                                                   verbose=0,\r\n","                                                   save_best_only=True,\r\n","                                                   save_weights_only=True)\r\n","callbacks.append(ckpt_callback)\r\n","\r\n","# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\r\n","early_stop = True\r\n","if early_stop:\r\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\r\n","  callbacks.append(es_callback)\r\n","\r\n","# Fitting the model\r\n","# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\r\n","vqa_model_3.fit(train_dataset,\r\n","                epochs=100,\r\n","                steps_per_epoch=(len(dataset) // bs),\r\n","                validation_data=valid_dataset,\r\n","                validation_steps=(len(dataset_valid) // bs),\r\n","                callbacks=callbacks)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PECwvYGPio7t"},"source":["# Loading the best weights of the trained model\r\n","full_path = os.path.join('/content/drive/My Drive/AN2DL/VisualQuestionAnswering/vqa_experiments', exp_dir)\r\n","latest = tf.train.latest_checkpoint(full_path)\r\n","vqa_model_3.load_weights(latest)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5Bh4DtVWSgc"},"source":["# Checking how the model predictions on the validation set\r\n","import time\r\n","from matplotlib import cm\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline\r\n","\r\n","iterator = iter(valid_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wzuSsNJHWUsJ"},"source":["# Visualizing a target for an item in the validation set and its corresponding model prediction\r\n","(image, question), target = next(iterator)\r\n","\r\n","image = image[0]\r\n","question = question[0]\r\n","target = target[0]\r\n","\r\n","x = tf.expand_dims(image,0)\r\n","y = tf.expand_dims(question, 0)\r\n","out_sigmoid = vqa_model_3.predict([x,y])\r\n","predicted_class = tf.argmax(out_sigmoid, -1)\r\n","\r\n","print(\"Target:\")\r\n","tf.print(target)\r\n","print(\"Prediction:\")\r\n","print(predicted_class.numpy()[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVxKCJDWirAt"},"source":["# Loading the json file related to the test set, saving the items IDs, the questions and the images IDs in some lists\r\n","id_test = []\r\n","question_input_test = []\r\n","image_ids_test = []\r\n","\r\n","with open('/content/VQA_Dataset/test_questions.json') as json_file:\r\n","  test_json = json.load(json_file)                                         \r\n","\r\n","  for item in test_json:\r\n","    id_test.append(item)\r\n","    question_input_test.append(test_json[item]['question'])\r\n","    image_ids_test.append(test_json[item]['image_id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eG5H3nvXitHO"},"source":["# Using the same Tokenizer of the training part to convert the words in the questions to integers\r\n","question_tokenized_test = question_tokenizer.texts_to_sequences(question_input_test)\r\n","\r\n","# Padding to the same max question length used for training\r\n","question_encoder_inputs_test = pad_sequences(question_tokenized_test, maxlen=max_question_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTT1ppBNiume"},"source":["# Computing the predictions on the test set, giving each image with the related question to the model and computing the argmax of the output\r\n","# In particular, each image is first resized to the same dimensions used during training\r\n","# The predicted output is then saved into the results_test list\r\n","results_test = []\r\n","\r\n","for index, question in enumerate(question_encoder_inputs_test):\r\n","  img_test = Image.open(os.path.join(images_dir, image_ids_test[index] + '.png')).convert('RGB').resize((img_w, img_h))\r\n","  img_arr_test = np.array(img_test)\r\n","  img_arr_test = preprocess_input(img_arr_test)\r\n","  img_arr_test = tf.expand_dims(img_arr_test, 0)\r\n","  \r\n","  question = tf.expand_dims(question, 0)\r\n","  \r\n","  prediction = vqa_model_3.predict([img_arr_test, question])\r\n","  results_test.append(tf.argmax(prediction, -1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhKZJId7ixEb"},"source":["# Creating the dictionary to be written in the output csv file\r\n","# It will contain the ID of each test item as the key, and the corresponding predicted output as the value\r\n","dictionary = {}\r\n","\r\n","for index, id in enumerate(id_test):\r\n","  dictionary[id] = results_test[index].numpy()[0]\r\n","\r\n","# Exporting the dictionary created into a csv file\r\n","create_csv(dictionary, '/content/drive/My Drive/AN2DL/VisualQuestionAnswering/')"],"execution_count":null,"outputs":[]}]}